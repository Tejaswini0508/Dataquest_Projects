{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Spam Filter with Naive Bayes\n",
    "\n",
    "In this project we will be building a spam filter for SMS messages using Naive Bayes algorithm.\n",
    "\n",
    "To classify messages as spam or non spam:\n",
    "\n",
    "1. Learn how humans classify messages.\n",
    "2. Use that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "3. Classifiy a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal,then we may need a human to classify the message)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Humans Classify Messages\n",
    "\n",
    "As stated above our first task is to teach the computer how to classify messages. \n",
    "\n",
    "To do that we will use the multinomial Naive Bayes algorithm with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection).The data collection process is described in more details [on this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "data=pd.read_csv(\"SMSSpamCollection\",sep='\\t',header=None,\n",
    "                 names=['Label','SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.865937\n",
       "spam    0.134063\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of spam and non spam messages\n",
    "#ham means non spam\n",
    "data[\"Label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have approximatelt 87% non spam and 13% spam messages in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into training and testing set\n",
    "\n",
    "Now that we have explored the dataset we can start buulding our spam filter.\n",
    "However we should also be able to test our new filter to see how it works.\n",
    "For this purpose we will first split the data into training and testing sets.\n",
    "The training set will have 80% of the data while the testing set will have 20% of the data.\n",
    "\n",
    "The model will be build on the training  dataset.We will treat the data in testing set as new messages and test our model which we build on training set. If we are able to classify more than 80% of our messages correctly in the test set then we can say our model is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the entire dataset\n",
    "data_random=data.sample(frac=1,random_state=1)\n",
    "\n",
    "#Split the above randomized data into training and testing set\n",
    "train_set=data_random.sample(frac=0.8,random_state=1)\n",
    "\n",
    "# Drop whatever is classified as train set from the test set\n",
    "test_set=data_random.drop(train_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the indexes for both train and test set\n",
    "train_set.reset_index(inplace=True,drop=True)\n",
    "\n",
    "test_set.reset_index(inplace=True,drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.866756\n",
       "spam    0.133244\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the percentage of spam and non spam in both train and test sets\n",
    "\n",
    "train_set[\"Label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     0.862657\n",
       "spam    0.137343\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[\"Label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the original ratio of 87% non spam to 13% spam of original dataset have been retained in both train and sets.Now we can start building the model on training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Overview\n",
    "\n",
    "The Naive Bayes algorithm(Multinomial Naive Bayes) works in the following way:\n",
    "\n",
    "P(wi|Spam)=(Nwi|Spam+α)/(NSpam+α⋅NVocabulary)\n",
    "\n",
    "P(wi|Ham)=(Nwi|Ham+α)/(NHam+α⋅NVocabulary)\n",
    "\n",
    "The Naive Bayes equations are:\n",
    "\n",
    "P(Spam|w1,w2,...,wn)∝P(Spam) * Π(wi|Spam) for i=1,2,...n\n",
    "\n",
    "P(Ham|w1,w2,...,wn)∝P(Ham) * Π(wi|Ham)\n",
    "\n",
    "* P(wi|Spam)=Probability that given the message is spam the word wi is present in it.\n",
    "* P(wi|Ham)=Probability that given the message is ham the word wi is present in it.\n",
    "* Π=Product\n",
    "* Nwi|Spam=the number of times the word wi occurs in spam messages\n",
    "* Nwi|SpamC=the number of times the word wi occurs in non-spam messages\n",
    "* NSpam=total number of words in spam messages\n",
    "* NSpamC=total number of words in non-spam messages\n",
    "* NVocabulary=total number of words in the vocabulary\n",
    "* α=1    (α is a smoothing parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Cleaning and Preparation\n",
    "In order to calculate the above probabilities we will need to arrange the dataset in a specific format.\n",
    "\n",
    "For example if the data is as below:\n",
    "\n",
    "| Label \t| SMS                                    \t|\n",
    "|-------\t|----------------------------------------\t|\n",
    "| spam  \t| SECRET PRIZE!!CLAIM SECRET PRIZE NOW!! \t|\n",
    "| ham   \t| Coming to my secret part?              \t|\n",
    "| spam  \t| Winner!Claim secret prize now!         \t|\n",
    "\n",
    "\n",
    "It should be brought in the below format\n",
    "\n",
    "| Label \t| secret \t| prize \t| claim \t| now  \t| coming \t| to \t| my  \t| party \t| winner \t|\n",
    "|-------\t|--------\t|-------\t|-------\t|------\t|--------\t|----\t|-----\t|-------\t|--------\t|\n",
    "| spam  \t| 2      \t| 2     \t| 1     \t| 1    \t| 0      \t| 0  \t| 0   \t| 0     \t| 0      \t|\n",
    "| ham   \t| 1      \t| 0     \t| 0     \t| 0    \t| 1      \t| 1  \t| 1   \t| 1     \t| 0      \t|\n",
    "| spam  \t| 1      \t| 1     \t| 1     \t| 1    \t| 0      \t| 0  \t| 0   \t| 0     \t| 1      \t|\n",
    "\n",
    "In the transformation above:\n",
    "\n",
    "* The SMS column doesn't exist anymore.\n",
    " Instead, the SMS column is replaced by a series of new columns, where each column represents a unique word from the vocabulary.\n",
    "* Each row describes a single message. For instance, the first row corresponds to the message \"SECRET PRIZE! CLAIM SECRET PRIZE NOW!!\", and it has the values spam, 2, 2, 1, 1, 0, 0, 0, 0, 0. These values tell us that:\n",
    "  * The message is spam.\n",
    "  * The word \"secret\" occurs two times inside the message.\n",
    "  * The word \"prize\" occurs two times inside the message.\n",
    "  * The word \"claim\" occurs one time inside the message.\n",
    "  * The word \"now\" occurs one time inside the message.\n",
    "  * The words \"coming\", \"to\", \"my\", \"party\", and \"winner\" occur zero times inside the message.\n",
    "* All words in the vocabulary are in lower case, so \"SECRET\" and \"secret\" come to be considered to be the same word.\n",
    "* Punctuation is not taken into account anymore (for instance, we can't look at the table and conclude that the first message initially had three exclamation marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Remove the punctuations and convert the case to lower\n",
    "\n",
    "\n",
    "train_set[\"clean_SMS\"]=train_set[\"SMS\"].str.replace(\"\\W\",\" \")\n",
    "'''\n",
    "\\W is a regex pattern which will replace any character which is not\n",
    "in a-z,A-Z or 0-9\n",
    "'''\n",
    "train_set[\"clean_SMS\"]=train_set[\"clean_SMS\"].str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done with cleaning the data we will need to transform the data as we stated above. The individual columns are nothing but unique words from all the words in SMS column and the value is the frequency of that particular word.We will refer to this unique set of words as **vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "\n",
    "# Transform the sms column into a list of words\n",
    "train_set['clean_sms_list']=train_set[\"clean_SMS\"].str.split()\n",
    "\n",
    "vocabulary=[]\n",
    "\n",
    "for i in train_set[\"clean_sms_list\"]:\n",
    "    for word in i:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>clean_SMS</th>\n",
       "      <th>clean_sms_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Good night my dear.. Sleepwell&amp;amp;Take care</td>\n",
       "      <td>good night my dear   sleepwell amp take care</td>\n",
       "      <td>[good, night, my, dear, sleepwell, amp, take, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sen told that he is going to join his uncle fi...</td>\n",
       "      <td>sen told that he is going to join his uncle fi...</td>\n",
       "      <td>[sen, told, that, he, is, going, to, join, his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Thank you baby! I cant wait to taste the real ...</td>\n",
       "      <td>thank you baby  i cant wait to taste the real ...</td>\n",
       "      <td>[thank, you, baby, i, cant, wait, to, taste, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>When can ü come out?</td>\n",
       "      <td>when can ü come out</td>\n",
       "      <td>[when, can, ü, come, out]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>No. Thank you. You've been wonderful</td>\n",
       "      <td>no  thank you  you ve been wonderful</td>\n",
       "      <td>[no, thank, you, you, ve, been, wonderful]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  \\\n",
       "0   ham       Good night my dear.. Sleepwell&amp;Take care   \n",
       "1   ham  Sen told that he is going to join his uncle fi...   \n",
       "2   ham  Thank you baby! I cant wait to taste the real ...   \n",
       "3   ham                               When can ü come out?   \n",
       "4   ham               No. Thank you. You've been wonderful   \n",
       "\n",
       "                                           clean_SMS  \\\n",
       "0       good night my dear   sleepwell amp take care   \n",
       "1  sen told that he is going to join his uncle fi...   \n",
       "2  thank you baby  i cant wait to taste the real ...   \n",
       "3                               when can ü come out    \n",
       "4               no  thank you  you ve been wonderful   \n",
       "\n",
       "                                      clean_sms_list  \n",
       "0  [good, night, my, dear, sleepwell, amp, take, ...  \n",
       "1  [sen, told, that, he, is, going, to, join, his...  \n",
       "2  [thank, you, baby, i, cant, wait, to, taste, t...  \n",
       "3                          [when, can, ü, come, out]  \n",
       "4         [no, thank, you, you, ve, been, wonderful]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7712"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'night',\n",
       " 'my',\n",
       " 'dear',\n",
       " 'sleepwell',\n",
       " 'amp',\n",
       " 'take',\n",
       " 'care',\n",
       " 'sen',\n",
       " 'told',\n",
       " 'that',\n",
       " 'he',\n",
       " 'is',\n",
       " 'going',\n",
       " 'to',\n",
       " 'join',\n",
       " 'his',\n",
       " 'uncle',\n",
       " 'finance',\n",
       " 'in',\n",
       " 'cbe',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'baby',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'wait',\n",
       " 'taste',\n",
       " 'the',\n",
       " 'real',\n",
       " 'thing',\n",
       " 'when',\n",
       " 'can',\n",
       " 'ü',\n",
       " 'come',\n",
       " 'out',\n",
       " 'no',\n",
       " 've',\n",
       " 'been',\n",
       " 'wonderful',\n",
       " 'watching',\n",
       " 'telugu',\n",
       " 'movie',\n",
       " 'wat',\n",
       " 'abt',\n",
       " 'u',\n",
       " 'get',\n",
       " 'ready',\n",
       " 'moan',\n",
       " 'and',\n",
       " 'scream',\n",
       " 'babe',\n",
       " 'miiiiiiissssssssss',\n",
       " 'need',\n",
       " 'crave',\n",
       " 'geeee',\n",
       " 'm',\n",
       " 'so',\n",
       " 'sad',\n",
       " 'without',\n",
       " 'love',\n",
       " 'up',\n",
       " 'wan',\n",
       " 'then',\n",
       " 'lor',\n",
       " 'but',\n",
       " 'din',\n",
       " 'c',\n",
       " 'any',\n",
       " 'stripes',\n",
       " 'skirt',\n",
       " 'sent',\n",
       " 'wife',\n",
       " 'your',\n",
       " 'text',\n",
       " 'after',\n",
       " 'we',\n",
       " 'buy',\n",
       " 'them',\n",
       " 'she',\n",
       " 'll',\n",
       " 'tell',\n",
       " 'what',\n",
       " 'do',\n",
       " 'just',\n",
       " 'relax',\n",
       " 'should',\n",
       " 'go',\n",
       " 'this',\n",
       " 'wkend',\n",
       " 'want',\n",
       " 'send',\n",
       " 'something',\n",
       " 'sell',\n",
       " 'fast',\n",
       " 'lt',\n",
       " 'gt',\n",
       " 'k',\n",
       " 'not',\n",
       " 'easy',\n",
       " 'money',\n",
       " 'class',\n",
       " 'did',\n",
       " 'alright',\n",
       " 'omw',\n",
       " 'gotta',\n",
       " 'change',\n",
       " 'order',\n",
       " 'a',\n",
       " 'half8th',\n",
       " 'home',\n",
       " 'all',\n",
       " 'creepy',\n",
       " 'crazy',\n",
       " 'me',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'ask',\n",
       " 'brother',\n",
       " 'nothing',\n",
       " 'problem',\n",
       " 'some',\n",
       " 'there',\n",
       " 'way',\n",
       " 'shade',\n",
       " 's',\n",
       " 'stuff',\n",
       " 'her',\n",
       " 'has',\n",
       " 'too',\n",
       " '2',\n",
       " 'little',\n",
       " 'pocy',\n",
       " 'bell',\n",
       " 'am',\n",
       " 'sorry',\n",
       " 'sounds',\n",
       " 'great',\n",
       " 'are',\n",
       " 'now',\n",
       " '123',\n",
       " 'congratulations',\n",
       " 'week',\n",
       " 'competition',\n",
       " 'draw',\n",
       " 'have',\n",
       " 'won',\n",
       " '1450',\n",
       " 'prize',\n",
       " 'claim',\n",
       " 'call',\n",
       " '09050002311',\n",
       " 'b4280703',\n",
       " 't',\n",
       " 'cs',\n",
       " 'stop',\n",
       " 'sms',\n",
       " '08718727868',\n",
       " 'over',\n",
       " '18',\n",
       " 'only',\n",
       " '150ppm',\n",
       " 'dunno',\n",
       " 'dad',\n",
       " 'said',\n",
       " 'coming',\n",
       " 'bring',\n",
       " 'us',\n",
       " '4',\n",
       " 'lunch',\n",
       " 'yup',\n",
       " 'w',\n",
       " 'reach',\n",
       " 'school',\n",
       " 'slave',\n",
       " 'or',\n",
       " '3',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'yourself',\n",
       " 'today',\n",
       " 'bright',\n",
       " 'light',\n",
       " 'on',\n",
       " 'cell',\n",
       " 'phone',\n",
       " 'think',\n",
       " 'anyone',\n",
       " 'with',\n",
       " 'spare',\n",
       " 'room',\n",
       " 'off',\n",
       " 'top',\n",
       " 'head',\n",
       " 'cps',\n",
       " 'causing',\n",
       " 'outages',\n",
       " 'conserve',\n",
       " 'energy',\n",
       " 'hanging',\n",
       " 'family',\n",
       " 'lasting',\n",
       " 'as',\n",
       " 'much',\n",
       " 'hours',\n",
       " 'might',\n",
       " 'lucky',\n",
       " 'here',\n",
       " 'discount',\n",
       " 'code',\n",
       " 'rp176781',\n",
       " 'further',\n",
       " 'messages',\n",
       " 'reply',\n",
       " 'www',\n",
       " 'regalportfolio',\n",
       " 'co',\n",
       " 'uk',\n",
       " 'customer',\n",
       " 'services',\n",
       " '08717205546',\n",
       " 'it',\n",
       " 'happens',\n",
       " 'r',\n",
       " '2waxsto',\n",
       " 'ill',\n",
       " 'medical',\n",
       " 'insurance',\n",
       " 'be',\n",
       " 'able',\n",
       " 'deliver',\n",
       " 'basic',\n",
       " 'currently',\n",
       " 'shopping',\n",
       " 'for',\n",
       " 'right',\n",
       " 'give',\n",
       " 'til',\n",
       " 'friday',\n",
       " 'morning',\n",
       " 'thats',\n",
       " 'see',\n",
       " 'major',\n",
       " 'person',\n",
       " 'guide',\n",
       " 'okie',\n",
       " 'y',\n",
       " 'sure',\n",
       " 'sick',\n",
       " 'time',\n",
       " 'excellent',\n",
       " 'spent',\n",
       " 'years',\n",
       " 'air',\n",
       " 'force',\n",
       " 'iraq',\n",
       " 'afghanistan',\n",
       " 'stable',\n",
       " 'honest',\n",
       " 'like',\n",
       " 'traveling',\n",
       " 'its',\n",
       " 'find',\n",
       " 'forgot',\n",
       " 'smth',\n",
       " 'card',\n",
       " 'da',\n",
       " 'present',\n",
       " 'lei',\n",
       " 'how',\n",
       " 'write',\n",
       " 'sign',\n",
       " 'yes',\n",
       " 'sura',\n",
       " 'sun',\n",
       " 'tv',\n",
       " 'lol',\n",
       " 'objection',\n",
       " 'bf',\n",
       " 'mm',\n",
       " 'fun',\n",
       " 'hey',\n",
       " 'mate',\n",
       " 'hows',\n",
       " 'honey',\n",
       " 'ave',\n",
       " 'holiday',\n",
       " 'gimmi',\n",
       " 'de',\n",
       " 'goss',\n",
       " 'x',\n",
       " 'anything',\n",
       " 'lar',\n",
       " 'inclusive',\n",
       " 'credits',\n",
       " 'pls',\n",
       " 'gotto',\n",
       " 'comuk',\n",
       " 'net',\n",
       " 'login',\n",
       " '3qxj9',\n",
       " 'unsubscribe',\n",
       " 'extra',\n",
       " 'charge',\n",
       " 'help',\n",
       " '08702840625',\n",
       " '220cm2',\n",
       " '9ae',\n",
       " 'jot',\n",
       " 'down',\n",
       " 'things',\n",
       " 'remember',\n",
       " 'later',\n",
       " 'long',\n",
       " 'quit',\n",
       " '5',\n",
       " 'minutes',\n",
       " 'day',\n",
       " 'maybe',\n",
       " 'fat',\n",
       " 'fingers',\n",
       " 'press',\n",
       " 'these',\n",
       " 'buttons',\n",
       " 'doesn',\n",
       " 'dnt',\n",
       " 'wnt',\n",
       " 'tlk',\n",
       " 'wid',\n",
       " 'aww',\n",
       " 'must',\n",
       " 'nearly',\n",
       " 'dead',\n",
       " 'well',\n",
       " 'jez',\n",
       " 'iscoming',\n",
       " 'todo',\n",
       " 'workand',\n",
       " 'whilltake',\n",
       " 'forever',\n",
       " 'ur',\n",
       " 'msg',\n",
       " 'yar',\n",
       " 'poor',\n",
       " 'one',\n",
       " 'tmr',\n",
       " 'brand',\n",
       " 'new',\n",
       " 'sleep',\n",
       " 'lousy',\n",
       " 'run',\n",
       " 'already',\n",
       " 'back',\n",
       " 'half',\n",
       " 'hee',\n",
       " 'urgent',\n",
       " 'trying',\n",
       " 'contact',\n",
       " 'last',\n",
       " 'weekends',\n",
       " 'shows',\n",
       " '1000',\n",
       " 'guaranteed',\n",
       " '09064017295',\n",
       " 'k52',\n",
       " 'valid',\n",
       " '12hrs',\n",
       " '150p',\n",
       " 'pm',\n",
       " 'company',\n",
       " 'very',\n",
       " 'environment',\n",
       " 'terrific',\n",
       " 'food',\n",
       " 'really',\n",
       " 'nice',\n",
       " 'mind',\n",
       " 'if',\n",
       " 'into',\n",
       " 'bedroom',\n",
       " 'minute',\n",
       " 'ok',\n",
       " 'sed',\n",
       " 'sexy',\n",
       " 'mood',\n",
       " 'came',\n",
       " 'minuts',\n",
       " 'latr',\n",
       " 'cake',\n",
       " 'n',\n",
       " 'thts',\n",
       " 'god',\n",
       " 'gift',\n",
       " 'birds',\n",
       " 'humans',\n",
       " 'hav',\n",
       " 'natural',\n",
       " 'frm',\n",
       " 'gud',\n",
       " 'mrng',\n",
       " 'meet',\n",
       " 'hard',\n",
       " 'true',\n",
       " 'show',\n",
       " 'express',\n",
       " 'someone',\n",
       " 'will',\n",
       " 'hurt',\n",
       " 'they',\n",
       " 'leave',\n",
       " 'seperated',\n",
       " '鈥',\n",
       " '〨ud',\n",
       " 'evening',\n",
       " 'thanx',\n",
       " 'got',\n",
       " 'exams',\n",
       " 'march',\n",
       " 'ive',\n",
       " 'done',\n",
       " 'revision',\n",
       " 'fran',\n",
       " 'still',\n",
       " 'boyf',\n",
       " 'interviw',\n",
       " 'exeter',\n",
       " 'bit',\n",
       " 'worried',\n",
       " 'nelson',\n",
       " 'bb',\n",
       " 'longer',\n",
       " 'comin',\n",
       " 'was',\n",
       " 'expecting',\n",
       " 'aint',\n",
       " 'ditto',\n",
       " 'worry',\n",
       " 'about',\n",
       " 'saying',\n",
       " 'anymore',\n",
       " 'whatever',\n",
       " 'same',\n",
       " 'peace',\n",
       " 'went',\n",
       " 'hon',\n",
       " 'lab',\n",
       " 'dude',\n",
       " 'seeing',\n",
       " 'lotta',\n",
       " 'corvettes',\n",
       " 'lately',\n",
       " 'aren',\n",
       " 'next',\n",
       " 'imma',\n",
       " 'flip',\n",
       " 'shit',\n",
       " 'battery',\n",
       " 'died',\n",
       " 'yeah',\n",
       " 'finally',\n",
       " 'lastest',\n",
       " 'from',\n",
       " 'stereophonics',\n",
       " 'marley',\n",
       " 'dizzee',\n",
       " 'racal',\n",
       " 'libertines',\n",
       " 'strokes',\n",
       " 'win',\n",
       " 'nookii',\n",
       " 'games',\n",
       " 'flirt',\n",
       " 'click',\n",
       " 'themob',\n",
       " 'wap',\n",
       " 'bookmark',\n",
       " '82468',\n",
       " 'presnts',\n",
       " 'always',\n",
       " 'bcz',\n",
       " 'mis',\n",
       " 'jeevithathile',\n",
       " 'irulinae',\n",
       " 'neekunna',\n",
       " 'prakasamanu',\n",
       " 'sneham',\n",
       " 'prakasam',\n",
       " 'ennal',\n",
       " 'prabha',\n",
       " 'mns',\n",
       " 'oh',\n",
       " 'ya',\n",
       " 'hip',\n",
       " 'hop',\n",
       " 'open',\n",
       " 'haha',\n",
       " 'thinking',\n",
       " 'jazz',\n",
       " 'zoom',\n",
       " 'cine',\n",
       " 'actually',\n",
       " 'tonight',\n",
       " 'free',\n",
       " 'leh',\n",
       " 'kb',\n",
       " 'lesson',\n",
       " 'puppy',\n",
       " 'noise',\n",
       " 'meant',\n",
       " 'calculation',\n",
       " 'units',\n",
       " 'at',\n",
       " 'expensive',\n",
       " 'started',\n",
       " 'practicing',\n",
       " 'accent',\n",
       " 'because',\n",
       " 'important',\n",
       " 'decided',\n",
       " 'doing',\n",
       " '4years',\n",
       " 'dental',\n",
       " 'nmde',\n",
       " 'exam',\n",
       " 'sindu',\n",
       " 'job',\n",
       " 'birla',\n",
       " 'soft',\n",
       " 'our',\n",
       " 'ride',\n",
       " 'equally',\n",
       " 'uneventful',\n",
       " 'many',\n",
       " 'those',\n",
       " 'pesky',\n",
       " 'cyclists',\n",
       " 'around',\n",
       " 'textin',\n",
       " 'bout',\n",
       " 're',\n",
       " 'getting',\n",
       " 'derek',\n",
       " 'taylor',\n",
       " 'assumed',\n",
       " 'worst',\n",
       " 'successful',\n",
       " 'boltblue',\n",
       " 'tones',\n",
       " 'poly',\n",
       " 'mono',\n",
       " 'eg',\n",
       " 'poly3',\n",
       " '1',\n",
       " 'cha',\n",
       " 'slide',\n",
       " 'slow',\n",
       " 'jamz',\n",
       " '6',\n",
       " 'toxic',\n",
       " '8',\n",
       " 'more',\n",
       " 'txt',\n",
       " 'message',\n",
       " 'activate',\n",
       " '500',\n",
       " 'by',\n",
       " 'replying',\n",
       " 'word',\n",
       " 'terms',\n",
       " 'conditions',\n",
       " 'visit',\n",
       " '07781482378',\n",
       " 'com',\n",
       " 'nobody',\n",
       " 'decide',\n",
       " 'where',\n",
       " 'eat',\n",
       " 'wants',\n",
       " 'chinese',\n",
       " 'power',\n",
       " 'supplies',\n",
       " 'needy',\n",
       " 'pouts',\n",
       " 'stomps',\n",
       " 'feet',\n",
       " 'him',\n",
       " 'yay',\n",
       " 'missed',\n",
       " 'cinema',\n",
       " 'trip',\n",
       " 'happy',\n",
       " 'year',\n",
       " 'having',\n",
       " 'coffee',\n",
       " 'wif',\n",
       " 'frens',\n",
       " 'fren',\n",
       " 'drove',\n",
       " 'lift',\n",
       " 'awarded',\n",
       " 'city',\n",
       " 'break',\n",
       " 'could',\n",
       " '200',\n",
       " 'summer',\n",
       " 'spree',\n",
       " 'every',\n",
       " 'wk',\n",
       " 'store',\n",
       " '88039',\n",
       " 'skilgme',\n",
       " 'tscs087147403231winawk',\n",
       " 'age16',\n",
       " '50perwksub',\n",
       " 'nigh',\n",
       " 'aha',\n",
       " 'image',\n",
       " 'month',\n",
       " 'nor',\n",
       " 'mid',\n",
       " 'look',\n",
       " 'frying',\n",
       " 'pan',\n",
       " 'case',\n",
       " 'cheap',\n",
       " 'book',\n",
       " 'perhaps',\n",
       " 'silly',\n",
       " 'isn',\n",
       " 'likely',\n",
       " 'use',\n",
       " 'foreign',\n",
       " 'stamps',\n",
       " 'country',\n",
       " 'lecture',\n",
       " 'nah',\n",
       " 'wednesday',\n",
       " 'mini',\n",
       " 'cheetos',\n",
       " 'bag',\n",
       " 'best',\n",
       " 'congrats',\n",
       " 'line',\n",
       " 'till',\n",
       " 'forget',\n",
       " 'realize',\n",
       " 'cannot',\n",
       " 'gn',\n",
       " 'im',\n",
       " 'computerless',\n",
       " 'make',\n",
       " 'oreo',\n",
       " 'truffles',\n",
       " 'kind',\n",
       " 'train',\n",
       " 'cos',\n",
       " 'asthma',\n",
       " 'attack',\n",
       " 'nxt',\n",
       " 'hr',\n",
       " 'driving',\n",
       " 'park',\n",
       " 'timin',\n",
       " 'until',\n",
       " 'fighting',\n",
       " 'world',\n",
       " 'either',\n",
       " 'lose',\n",
       " 'bt',\n",
       " 'fightng',\n",
       " 'some1',\n",
       " 'who',\n",
       " 'close',\n",
       " 'dificult',\n",
       " 'anytime',\n",
       " 'took',\n",
       " 'zaher',\n",
       " 'words',\n",
       " 'ym',\n",
       " 'had',\n",
       " 'left',\n",
       " 'miss',\n",
       " 'rose',\n",
       " 'red',\n",
       " 'blood',\n",
       " 'heart',\n",
       " 'tis',\n",
       " 'friends',\n",
       " 'including',\n",
       " 'relation',\n",
       " 'support',\n",
       " 'frnd',\n",
       " 'luvs',\n",
       " 'praying',\n",
       " 'marry',\n",
       " 'try',\n",
       " 'wake',\n",
       " 'wondering',\n",
       " 'grandma',\n",
       " 'before',\n",
       " 'parade',\n",
       " 'knw',\n",
       " 'dis',\n",
       " 'wa',\n",
       " 'efficient',\n",
       " 'gee',\n",
       " 'guys',\n",
       " 'g',\n",
       " 'were',\n",
       " 'staying',\n",
       " 'mcr',\n",
       " 'paragon',\n",
       " 'havent',\n",
       " 'whether',\n",
       " 'cut',\n",
       " 'yet',\n",
       " 'imagine',\n",
       " 'would',\n",
       " 'gentle',\n",
       " 'unlike',\n",
       " 'other',\n",
       " 'docs',\n",
       " 'treat',\n",
       " 'their',\n",
       " 'patients',\n",
       " 'turkeys',\n",
       " 'elaborating',\n",
       " 'safety',\n",
       " 'aspects',\n",
       " 'issues',\n",
       " 'jay',\n",
       " 'snickering',\n",
       " 'tells',\n",
       " 'totally',\n",
       " 'fucking',\n",
       " 'chords',\n",
       " 'speak',\n",
       " 'reminder',\n",
       " 'weekend',\n",
       " 'arrested',\n",
       " 'possession',\n",
       " 'since',\n",
       " 'which',\n",
       " 'side',\n",
       " 'fever',\n",
       " 'vomitin',\n",
       " 'least',\n",
       " 'armand',\n",
       " 'wasn',\n",
       " 'enough',\n",
       " 'trouble',\n",
       " 'sleeping',\n",
       " 'todays',\n",
       " '800',\n",
       " '09050001808',\n",
       " 'land',\n",
       " 'm95',\n",
       " 'valid12hrs',\n",
       " 'soryda',\n",
       " 'realy',\n",
       " 'sory',\n",
       " 'mark',\n",
       " 'taking',\n",
       " 'pick',\n",
       " 'prescription',\n",
       " 'pain',\n",
       " 'freemsg',\n",
       " 'darling',\n",
       " 'd',\n",
       " 'tb',\n",
       " 'xxx',\n",
       " 'std',\n",
       " 'chgs',\n",
       " '50',\n",
       " 'rcv',\n",
       " 'hi',\n",
       " 'sat',\n",
       " 'bloody',\n",
       " 'bus',\n",
       " 'mo',\n",
       " 'wont',\n",
       " '7',\n",
       " '30',\n",
       " 'wanna',\n",
       " 'somethin',\n",
       " 'ortxt',\n",
       " 'jess',\n",
       " 'xx',\n",
       " 'drug',\n",
       " 'don',\n",
       " 'goes',\n",
       " 'usf',\n",
       " 'lives',\n",
       " 'though',\n",
       " 'slaaaaave',\n",
       " 'summon',\n",
       " 'wish',\n",
       " 'own',\n",
       " 'plan',\n",
       " 'valentines',\n",
       " 'url',\n",
       " 'sweet',\n",
       " 'dreams',\n",
       " 'ringtone',\n",
       " 'sub',\n",
       " 'weekly',\n",
       " '1st',\n",
       " 'subpoly',\n",
       " '81618',\n",
       " 'per',\n",
       " '08718727870',\n",
       " 'nope',\n",
       " 'v',\n",
       " 'bored',\n",
       " 'thought',\n",
       " 'king',\n",
       " 'hill',\n",
       " 'wot',\n",
       " 'say',\n",
       " 'dust',\n",
       " 'monthly',\n",
       " 'amount',\n",
       " 'terrible',\n",
       " 'pay',\n",
       " '6months',\n",
       " 'finishing',\n",
       " 'nokia',\n",
       " 'tone',\n",
       " 'mob',\n",
       " 'nok',\n",
       " '87021',\n",
       " 'txtin',\n",
       " '16',\n",
       " 'hl',\n",
       " '4info',\n",
       " 'exactly',\n",
       " 'chechi',\n",
       " 'sense',\n",
       " 'foot',\n",
       " 'penis',\n",
       " 'news',\n",
       " 'hassling',\n",
       " 'weed',\n",
       " 'andres',\n",
       " 'haughaighgtujhyguj',\n",
       " 'mon',\n",
       " 'gd',\n",
       " 'la',\n",
       " 'ex',\n",
       " 'oso',\n",
       " 'depends',\n",
       " 'wana',\n",
       " 'western',\n",
       " 'den',\n",
       " 'prefer',\n",
       " 'madam',\n",
       " 'regret',\n",
       " 'disturbance',\n",
       " 'receive',\n",
       " 'reference',\n",
       " 'check',\n",
       " 'dlf',\n",
       " 'premarica',\n",
       " 'kindly',\n",
       " 'informed',\n",
       " 'rgds',\n",
       " 'rakhesh',\n",
       " 'kerala',\n",
       " 'spoke',\n",
       " 'mag',\n",
       " 'people',\n",
       " 'end',\n",
       " '24th',\n",
       " 'sept',\n",
       " 'talk',\n",
       " 'loan',\n",
       " 'purpose',\n",
       " '75',\n",
       " '000',\n",
       " 'homeowners',\n",
       " 'tenants',\n",
       " 'welcome',\n",
       " 'previously',\n",
       " 'refused',\n",
       " '0800',\n",
       " '1956669',\n",
       " 'erm',\n",
       " '45pm',\n",
       " 'chikku',\n",
       " 'favourite',\n",
       " 'song',\n",
       " 'ba',\n",
       " 'dao',\n",
       " 'never',\n",
       " 'ah',\n",
       " 'fri',\n",
       " 'kept',\n",
       " 'waking',\n",
       " 'cat',\n",
       " 'wanted',\n",
       " 'cold',\n",
       " 'va',\n",
       " 'mobile',\n",
       " '10',\n",
       " 'mths',\n",
       " 'update',\n",
       " 'latest',\n",
       " 'orange',\n",
       " 'camera',\n",
       " 'video',\n",
       " 'phones',\n",
       " 'save',\n",
       " 'texts',\n",
       " 'calls',\n",
       " 'callback',\n",
       " 'orno',\n",
       " 'opt',\n",
       " 'prolly',\n",
       " 'yo',\n",
       " 'gonna',\n",
       " 'stock',\n",
       " 'tomorrow',\n",
       " 'dubsack',\n",
       " 'matthew',\n",
       " 'please',\n",
       " '09063440451',\n",
       " 'landline',\n",
       " 'complimentary',\n",
       " 'lux',\n",
       " 'tenerife',\n",
       " 'cash',\n",
       " 'await',\n",
       " 'collection',\n",
       " 'ppm150',\n",
       " 'sae',\n",
       " 'box334',\n",
       " 'sk38xh',\n",
       " 'film',\n",
       " 'means',\n",
       " 'teju',\n",
       " 'cheers',\n",
       " 'zogtorius',\n",
       " 'staring',\n",
       " 'an',\n",
       " 'age',\n",
       " 'deciding',\n",
       " 'friend',\n",
       " 'studying',\n",
       " 'warwick',\n",
       " 'planned',\n",
       " 'concert',\n",
       " 'tmw',\n",
       " 'may',\n",
       " 'canceled',\n",
       " 'havn',\n",
       " 'seen',\n",
       " 'ages',\n",
       " 'together',\n",
       " 'sometime',\n",
       " 'stupid',\n",
       " 'challenge',\n",
       " 'instead',\n",
       " 'respond',\n",
       " 'immed',\n",
       " 'says',\n",
       " 'definitely',\n",
       " 'buying',\n",
       " 'starting',\n",
       " 'stay',\n",
       " 'weather',\n",
       " 'social',\n",
       " 'system',\n",
       " 'pansy',\n",
       " 'living',\n",
       " 'jungle',\n",
       " 'two',\n",
       " 'vijay',\n",
       " 'jaya',\n",
       " 'mum',\n",
       " 'lookin',\n",
       " 'strong',\n",
       " 'aft',\n",
       " 'dat',\n",
       " 'str',\n",
       " 'orchard',\n",
       " 'sir',\n",
       " 'waiting',\n",
       " 'letter',\n",
       " 'mistake',\n",
       " 'goto',\n",
       " 'doctor',\n",
       " 'ever',\n",
       " 'didn',\n",
       " 'laugh',\n",
       " 'embarassed',\n",
       " 'delete',\n",
       " 'tag',\n",
       " 'keep',\n",
       " 'far',\n",
       " 'knew',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 7712 unique words in the vocabulary of train set.\n",
    "\n",
    "Now that we have the vocabulary we will transform the dataset as desired.\n",
    "One approach is we create a dictionary with keys as the unique words in SMS with their respective values as the number of times the respective word appears in SMS.Eventually we can convert it to a dataframe so that we get the data in a desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "label = []\n",
    "word = []\n",
    "freq = []\n",
    "for j in range(len(vocabulary)):\n",
    "    for i in range(train_set.shape[0]):\n",
    "        label.append(train_set.iloc[i]['Label'])\n",
    "        word.append(vocabulary[j])\n",
    "        freq.append(train_set.iloc[i]['clean_sms_list'].count(vocabulary[j]))\n",
    "\n",
    "        \n",
    "data = list(zip(label,word, freq))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data, columns=['labels', 'word', 'value'])\n",
    "\n",
    " \n",
    "\n",
    "pvt = df.pivot('labels', 'word', 'value')\n",
    "\n",
    " \n",
    "\n",
    "reshaped_df = pvt.rename_axis(None, axis=1).reset_index()\n",
    "\n",
    " \n",
    "\n",
    "reshaped_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
