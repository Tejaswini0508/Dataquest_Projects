{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Winning Jeopardy\n",
    "\n",
    "[Jeopardy](https://en.wikipedia.org/wiki/Jeopardy!) is a popular TV show in the US where participants answer questions to win money. In this the participants are actually given answer clues and they are supposed to somi up with the questions.It's been running for many years now and is a major force in popular culture.\n",
    "\n",
    "Imagine that we want to compete on Jeopardy and we are looking for a way to win.In this project we will be dealing with a dataset that contains Jeopardy questions to figure out some patterns in the questions that could help us win.\n",
    "\n",
    "The dataset can be downloaded from [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#IRead the dataset\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"jeopardy.csv\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the explaination of the columns:\n",
    "\n",
    "| Column Name \t| Description                                       \t|\n",
    "|-------------\t|---------------------------------------------------\t|\n",
    "| Show Number \t| the Jeopardy episode number                       \t|\n",
    "| Air Date    \t| the date the episode aired                        \t|\n",
    "| Round       \t| the round of Jeopardy                             \t|\n",
    "| Category    \t| the category of the question                      \t|\n",
    "| Value       \t| the number of dollars the correct answer is worth \t|\n",
    "| Question    \t| the text of the question                          \t|\n",
    "| Answer      \t| the text of the answer                            \t|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the column names we can see the columns have spaces.Let's try dealing with those spaces first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
       "       ' Question', ' Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing the space in columns\n",
    "data.columns=[x.replace(' ','') for x in data.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ShowNumber', 'AirDate', 'Round', 'Category', 'Value', 'Question',\n",
       "       'Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of Text Columns\n",
    "\n",
    "Before we start doing any analysis on our columns we will need to normalize the text columns i.e. remove any punctuation and convert all the tect into either lower or upper case so that ABC and abc are not treated differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalization of question and answer column\n",
    "import re\n",
    "def normalize_text(word):\n",
    "    word=word.lower()\n",
    "    word = re.sub(\"[^A-Za-z0-9\\s]\", \"\", word)\n",
    "    word = re.sub(\"\\s+\", \" \", word)\n",
    "    return word\n",
    "\n",
    "data[\"clean_question\"]=data[\"Question\"].apply(normalize_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"clean_answer\"]=data[\"Answer\"].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of Other Columns\n",
    "\n",
    "Now that the text columns are normalized we can look at some other columns which can be normalized.\n",
    "\n",
    "The Value column should be numeric.It has a dollar sign.hence we will need to remove the same.\n",
    "\n",
    "The Air Date column should also be a datetime not a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_values(word):\n",
    "    word= word.lower()\n",
    "    word = re.sub(\"[^A-Za-z0-9\\s]\", \"\", word)\n",
    "    word = re.sub(\"\\s+\", \" \", word)\n",
    "    '''\n",
    "    Convert the word to an integer.\n",
    "    If it throws an error then make it 0\n",
    "    '''\n",
    "    try:\n",
    "        word=int(word)\n",
    "    except Exception:\n",
    "        word=0\n",
    "    return word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize the value column\n",
    "data[\"clean_value\"]=data[\"Value\"].apply(normalize_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert Air Date to datetime column\n",
    "\n",
    "data[\"AirDate\"]=pd.to_datetime(data[\"AirDate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore past questions\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study it all, it would be helpful to figure out two things:\n",
    "\n",
    "* How often the answer can be used for a question.\n",
    "* How often questions are repeated.\n",
    "\n",
    "We can answer the first question by seeing how many times words in the answer also occur in the question. \n",
    "\n",
    "We can answer the second question by seeing how often complex words (> 6 characters) reoccur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write a function to determine how many times words in the question also occur\n",
    "# in the answer.\n",
    "\n",
    "def count_matches(row):\n",
    "    split_answer=row[\"clean_answer\"].split( )\n",
    "    split_question=row[\"clean_question\"].split( )\n",
    "    match_count=0\n",
    "    if \"the\" in split_answer:\n",
    "        split_answer.remove(\"the\")\n",
    "    if len(split_answer)==0:\n",
    "        return 0\n",
    "    for word in split_answer:\n",
    "        if word in split_question:\n",
    "            match_count+=1\n",
    "    return match_count/len(split_answer)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Count how many times terms in clean_answer occur in clean_question.\n",
    "answer_in_question=data.apply(count_matches,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05900196524977763"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean of answer_in_question column\n",
    "answer_in_question.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that in 5% of the questions and answers have similar words in both.\n",
    "\n",
    "This can be taken as a input while coming up with the question formation in jeopardy. We can look for some hints in the answers to come up with the questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetition of Questions\n",
    "\n",
    "We want to Investigate how oftern new questions are repeats of older ones.\n",
    "This cannot be completely answered because we only have about 10% of the full Jeopardy question dataset but we can only investigate this.\n",
    "\n",
    "To do this we can:\n",
    "\n",
    "* Sort jeopardy in order of ascending air date.\n",
    "* Maintain a set called terms_used that will be empty initially.\n",
    "* Iterate through each row of jeopardy.\n",
    "* Split clean_question into words, remove any word shorter than 6 characters, and check if each word occurs in terms_used.\n",
    "* If it does, increment a counter.\n",
    "* Add each word to terms_used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing the function using the above steps\n",
    "\n",
    "question_overlap=[]\n",
    "terms_used=set()\n",
    "\n",
    "#Sort jeopardy by ascending date\n",
    "data.sort_values(\"AirDate\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We have already sorted columns as per date above.\n",
    "So now we will iterate over those rows directly to check if words are repeated.\n",
    "'''\n",
    "\n",
    "for i,row in data.iterrows():\n",
    "    split_question=row[\"clean_question\"].split(' ')\n",
    "    split_question=[k for k in split_question if len(k)>5]\n",
    "    match_count=0\n",
    "    for word in split_question:\n",
    "        if word in terms_used:\n",
    "            match_count+=1\n",
    "    for word in split_question:\n",
    "        terms_used.add(word)\n",
    "    if len(split_question)>0:\n",
    "        match_count/=len(split_question)\n",
    "    question_overlap.append(match_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6876260592169802"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"question_overlap\"] = question_overlap\n",
    "\n",
    "data[\"question_overlap\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the above result there is almost 70% overlap of words in new questions and words in old questions.But we are only considering words and not phrases. Hence this might be really insignificant. However we can surely look into the repetition of questions side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High and Low Value Questions\n",
    "\n",
    "We can consider focusing on the questions as per value and focus only on high value questions which will help us win more money on Jeopardy.\n",
    "\n",
    "We can analyze the terms corresponding to high value questions using a chi squared test.\n",
    "\n",
    "We can categorize the questions as below:\n",
    "\n",
    "* Low value -- Any row where Value is less than 800.\n",
    "* High value -- Any row where Value is greater than 800.\n",
    "\n",
    "We will then be able to loop through each of the terms from the last screen, terms_used, and:\n",
    "\n",
    "1. Find the number of low value questions the word occurs in.\n",
    "2. Find the number of high value questions the word occurs in.\n",
    "3. Find the percentage of questions the word occurs in.\n",
    "4. Based on the percentage of questions the word occurs in, find expected counts.\n",
    "5. Compute the chi squared value based on the expected counts and the observed counts for high and low value questions.\n",
    "\n",
    "We can then find the words with the biggest difference in usage between high and low value questions by selecting the words with the highest associated chi-squared values. Doing this for all words would tale significant time so we will only do it for a sample now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign the value as high or low\n",
    "\n",
    "data[\"high_value\"]=[1 if i>800 else 0 for i in data[\"clean_value\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a function that counts the words in high and low count\n",
    "\n",
    "def word_count(word):\n",
    "    low_count=0\n",
    "    high_count=0\n",
    "    for i,row in data.iterrows():\n",
    "        split_question=row[\"clean_question\"].split(' ')\n",
    "        if word in split_question:\n",
    "            if row[\"high_value\"]==1:\n",
    "                high_count+=1\n",
    "            else:\n",
    "                low_count=1\n",
    "    return high_count,low_count\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Randomly pick ten elements of terms_used and append them to a list called \n",
    "comparison_terms\n",
    "'''\n",
    "import random\n",
    "comparison_terms=list(random.sample(terms_used,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list observed_expected to get observed values\n",
    "observed_expected=[]\n",
    "\n",
    "for i in comparison_terms:\n",
    "    a=word_count(i)\n",
    "    observed_expected.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (2, 1),\n",
       " (0, 1),\n",
       " (3, 1),\n",
       " (2, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the expected counts and chi squared values\n",
    "high_value_count=len(data[\"high_value\"]==1)\n",
    "low_value_count=len(data[\"high_value\"]==0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "import numpy as np\n",
    "chi_squared=[]\n",
    "for i in observed_expected:\n",
    "    '''\n",
    "    Add up both items in the list '''\n",
    "    total=sum(i)\n",
    "    '''\n",
    "    Divide total by no of rows in data'''\n",
    "    \n",
    "    total_prop=total/data.shape[0]\n",
    "    exp_high_val=total_prop*high_value_count\n",
    "    exp_low_val=total_prop*low_value_count\n",
    "    observed = np.array([i[0], i[1]])\n",
    "    expected = np.array([exp_high_val, exp_low_val])\n",
    "    chi_squared.append(chisquare(observed, expected)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=1.0, pvalue=0.31731050786291404),\n",
       " Power_divergenceResult(statistic=1.0, pvalue=0.31731050786291404),\n",
       " Power_divergenceResult(statistic=1.0, pvalue=0.31731050786291404),\n",
       " Power_divergenceResult(statistic=1.6666666666666665, pvalue=0.19670560245894675),\n",
       " Power_divergenceResult(statistic=1.0, pvalue=0.31731050786291404),\n",
       " Power_divergenceResult(statistic=2.5, pvalue=0.11384629800665763),\n",
       " Power_divergenceResult(statistic=1.6666666666666665, pvalue=0.19670560245894675),\n",
       " Power_divergenceResult(statistic=1.0, pvalue=0.31731050786291404),\n",
       " Power_divergenceResult(statistic=1.0, pvalue=0.31731050786291404),\n",
       " Power_divergenceResult(statistic=1.0, pvalue=0.31731050786291404)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that none of the p values are less than he threshold of 0.05.Hence none of the chi squared values are significant. Therefore there is no significant difference of usgae of words between high or low values.Also when we checked the freq we could see that they are very low(less than 5).With such a low frequency we cannot gurantee the significance of the test."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
